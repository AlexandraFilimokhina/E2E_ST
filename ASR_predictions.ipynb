{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "dx0m6mbtlyb0n9ffieuk8ei"
   },
   "outputs": [],
   "source": [
    "#!:bash\n",
    "python3 -m pip install  git+https://github.com/NVIDIA/NeMo.git@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "15fce277boegivawzqwl7i"
   },
   "outputs": [],
   "source": [
    "%pip install ffmpeg \n",
    "%pip install braceexpand \n",
    "%pip install webdataset \n",
    "%pip install g2p_en \n",
    "%pip install frozendict \n",
    "%pip install unidecode \n",
    "%pip install torch_stft \n",
    "%pip install editdistance \n",
    "%pip install install sphfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "6gaw94ed5ripqcvfoplk1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import wget\n",
    "import copy\n",
    "import ffmpeg\n",
    "import tarfile\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from pathlib import PurePath\n",
    "from pydub import AudioSegment\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getcwd()\n",
    "dataset_name ='dev-clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellId": "2j41tcuv6optv0uwawc2ba"
   },
   "outputs": [],
   "source": [
    "def download_extract_libri(data_dir, dataset_name):\n",
    "    '''\n",
    "    downloading the tar file with dataset and extract it into the directory\n",
    "    '''\n",
    "    if not os.path.exists(os.path.join(data_dir, f'{dataset_name}.tar.gz')):\n",
    "        libri_url = f'http://www.openslr.org/resources/12/{dataset_name}.tar.gz'\n",
    "        libri_path = wget.download(libri_url, data_dir)\n",
    "        print(f\"Dataset in .tar format downloaded at: {libri_path}\")\n",
    "    else:\n",
    "        print(\"Tarfile already exists\")\n",
    "        libri_path = os.path.join(data_dir, f'{dataset_name}.tar.gz')\n",
    "    extracted_dir = os.path.join(data_dir, 'Librispeech')\n",
    "    if not os.path.exists(extracted_dir):\n",
    "        tar = tarfile.open(libri_path)\n",
    "        tar.extractall(path=data_dir)\n",
    "        print(f\"Tarfile extracted in {extracted_dir}\")\n",
    "    else:\n",
    "        print(f\"Tarfile already extracted in {extracted_dir}\")\n",
    "        \n",
    "def flac2wav(data_dir, dataset_name):\n",
    "    '''\n",
    "    converting flac to wav\n",
    "    '''\n",
    "    flac_list = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.flac'))  \n",
    "    for flac_path in tqdm(flac_list, position=0, leave=False):\n",
    "        wav_path = path[:-5] + '.wav'\n",
    "        os.system(f'ffmpeg -i {path} {wav_path}')\n",
    "        os.remove(path)\n",
    "        \n",
    "def build_manifest(data_dir, dataset_name, test_manifest=data_dir + '/LibriSpeech/test_manifest.json'):\n",
    "    '''\n",
    "    build_manifest(for training)\n",
    "    '''\n",
    "    transcripts = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.txt'))\n",
    "    with open(test_manifest, 'w') as out_file:\n",
    "        for trans_path in tqdm(transcripts, position=0, leave=False):\n",
    "            with open(trans_path, 'r') as file:\n",
    "                for line in file.readlines():   \n",
    "                    \n",
    "                    transcript = line.lower()[:-1]\n",
    "                    audio_name = transcript.split(' ')[0]\n",
    "                    text = transcript[len(transcript.split(' ')[0]) + 1:]\n",
    "                    path_to_folder, _ = trans_path.rsplit('/', 1)\n",
    "                    audio_path = os.path.join(path_to_folder, audio_name+'.wav')\n",
    "                    duration = librosa.core.get_duration(filename=audio_path)\n",
    "                    \n",
    "                    metadata = {\n",
    "                                \"audio_filepath\": audio_path,\n",
    "                                \"duration\": duration,\n",
    "                                \"text\": text\n",
    "                                }\n",
    "                    json.dump(metadata, out_file)\n",
    "                    out_file.write('\\n')\n",
    "                    \n",
    "def process_broken_files(data_dir, dataset_name):\n",
    "    '''\n",
    "    re convert broken files\n",
    "    '''\n",
    "    wav_files = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.wav'))\n",
    "    broken_files = [file for file in wav_files if os.path.getsize(file) < 100]\n",
    "    for f in broken_files:\n",
    "        path_to_folder = f.split('.wav')[0].split('train-clean-100')[1]\n",
    "        flac_audio = data_dir + f'/LibriSpeech/{dataset_name}/'+ path_to_folder+'.flac'\n",
    "        file_path = PurePath(flac_audio)\n",
    "        flac_tmp_audio_data = AudioSegment.from_file(file_path, file_path.suffix[1:])\n",
    "        flac_tmp_audio_data.export(f, format=\"wav\")\n",
    "        \n",
    "def save_translation(data_dir, dataset_name):\n",
    "    '''\n",
    "    save all translations in one file to download from datasphere\n",
    "    '''\n",
    "    transcripts_j = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.trans_jasper.txt'))\n",
    "    for j_path in transcripts_j:\n",
    "        with open(j_path, 'r') as j_file:\n",
    "            with open(f\"{dataset_name}-translation.txt\", \"a\") as out_file:\n",
    "                for line in j_file:\n",
    "                    out_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_extract_libri(data_dir, dataset_name)\n",
    "flac2wav(data_dir, dataset_name)\n",
    "process_broken_files(data_dir, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9ccs5o6lx6ks8g7jhqqbe"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "asr_model = nemo_asr.models.EncDecCTCModel.from_pretrained(model_name=\"Jasper10x5Dr-En\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "d08z5vvk3cr8lhr7oq5ig2"
   },
   "outputs": [],
   "source": [
    "#!L\n",
    "#make predictions with the loaded model\n",
    "ground_truth, hypothesis = [], []\n",
    "transcripts = glob.glob(os.path.join(data_dir, f'LibriSpeech/{dataset_name}/**/**/*.trans.txt'))\n",
    "for trans_path in tqdm(transcripts, position=0, leave=False):\n",
    "    with open(trans_path, 'r') as file:\n",
    "        trans_jasper_path = trans_path.rsplit('.', 1)[0]+'_jasper.txt'\n",
    "        with open(trans_jasper_path, 'w') as out_file:\n",
    "            for line in file.readlines():   \n",
    "                transcript = line.lower()[:-1]\n",
    "                audio_name = transcript.split(' ')[0]\n",
    "                text = transcript[len(transcript.split(' ')[0]) + 1:]\n",
    "                path_to_folder, _ = trans_path.rsplit('/', 1)\n",
    "                audio_path = os.path.join(path_to_folder, audio_name+'.wav')\n",
    "                \n",
    "                jasper_text = asr_model.transcribe(paths2audio_files=[audio_path])[0]\n",
    "                ground_truth.append(text)\n",
    "                hypothesis.append(jasper_text)\n",
    "                out_file.write(f'{audio_name} {jasper_text.upper()}\\n')\n",
    "                \n",
    "df = pd.DataFrame(zip(ground_truth, hypothesis))\n",
    "df.to_csv(f\"{dataset_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_translation(data_dir, dataset_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notebookId": "7b52fd8e-dcc8-4e6a-9cdb-728767fcb699"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
